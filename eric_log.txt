parsed input parameters:
{
  "output_json": "data.json", 
  "images_root": "/home/rz1/VisualLearningProj/dataset/parsed_dataset/YouTubeClips", 
  "input_json": "../dataset/msvd_caption.json", 
  "word_count_threshold": 5, 
  "max_length": 16, 
  "output_h5": "data.h5", 
  "num_test": 200, 
  "num_val": 100
}
example processed tokens:
['a', 'person', 'chops', 'garlic']
['lionel', 'richie', 'dancing', 'on', 'the', 'ceiling']
['a', 'man', 'is', 'loading', 'his', 'gun']
['a', 'man', 'is', 'playing', 'a', 'guitar', 'and', 'singing']
['a', 'bunch', 'of', 'ants', 'are', 'gathering', 'bits', 'of', 'food']
['a', 'man', 'slices', 'through', 'a', 'two', 'liter', 'plastic', 'bottle', 'of', 'soda', 'pop', 'with', 'a', 'sword']
['a', 'elephant', 'going', 'into', 'a', 'food', 'store']
['a', 'woman', 'is', 'stirring', 'grabs', 'in', 'a', 'pot']
['a', 'guy', 'is', 'shooting', 'a', 'gun']
['a', 'woman', 'is', 'cutting', 'shrimp', 'lengthwise', 'in', 'half']
top words and their counts:
(90891, 'a')
(44662, 'is')
(29577, 'the')
(23019, 'man')
(10111, 'woman')
(9570, 'on')
(9375, 'in')
(7832, 'playing')
(7201, 'are')
(6822, 'of')
(6597, 'and')
(6417, 'with')
(5000, 'person')
(4902, 'to')
(3794, 'into')
(3531, 'an')
(3513, 'two')
(3370, 'dog')
(3336, 'cat')
(3306, 'girl')
total words: 574162
number of bad words: 9266/12938 = 71.62%
number of words in vocab would be 3672
number of UNKs: 15916/574162 = 2.77%
max length sentence in raw data:  52
sentence length distribution (count, number of words):
 0:          0   0.000000%
 1:         21   0.025979%
 2:        309   0.382255%
 3:       2121   2.623831%
 4:       8106   10.027710%
 5:      12456   15.408976%
 6:      17678   21.868969%
 7:      12930   15.995349%
 8:       9262   11.457766%
 9:       6396   7.912316%
10:       3887   4.808501%
11:       2417   2.990004%
12:       1530   1.892721%
13:        983   1.216042%
14:        673   0.832550%
15:        476   0.588847%
16:        322   0.398337%
17:        278   0.343906%
18:        222   0.274630%
19:        176   0.217725%
20:        149   0.184324%
21:        112   0.138552%
22:         72   0.089069%
23:         63   0.077936%
24:         53   0.065565%
25:         37   0.045772%
26:         25   0.030927%
27:         15   0.018556%
28:         14   0.017319%
29:         13   0.016082%
30:          3   0.003711%
31:          8   0.009897%
32:          1   0.001237%
33:          7   0.008660%
34:          1   0.001237%
35:          2   0.002474%
36:          2   0.002474%
37:          2   0.002474%
38:          1   0.001237%
39:          2   0.002474%
40:          1   0.001237%
41:          2   0.002474%
42:          1   0.001237%
43:          2   0.002474%
44:          0   0.000000%
45:          1   0.001237%
46:          0   0.000000%
47:          2   0.002474%
48:          1   0.001237%
49:          0   0.000000%
50:          0   0.000000%
51:          0   0.000000%
52:          1   0.001237%
inserting the special UNK token
assigned 100 to val, 200 to test.
encoded captions to array of size  (80836, 16)
[u'WTf5EgVY5uU', u'66', u'75']
270
